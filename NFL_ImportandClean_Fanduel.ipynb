{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook, I'm doing some work to pull the stats out for specific games for the nfl throughout the year. These were all one string in the table I scraped from RotoGuru1.com, so this table pulls the information and then cleans it up into something I can use.\n",
    "\n",
    "Here is the page: http://rotoguru1.com/cgi-bin/fyday.pl?game=fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling in all the necessary python libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##realized the url just updated based on week number, so wrote a loop to accumulate all of the urls\n",
    "weeks = list(range(1,4,1))\n",
    "##every week added we just need to update this range\n",
    "\n",
    "urls = []\n",
    "for week in weeks:\n",
    "    urls.append(str('http://rotoguru1.com/cgi-bin/fyday.pl?week='+str(week)+'&game=fd'))\n",
    "\n",
    "##we should now have the page we need for each week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script to scrape all of the player data. I only ran this once and saved it as a csv in the section in the code marked below because running it is very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating an empty list that will have the player row for each table\n",
    "\n",
    "rows = []\n",
    "\n",
    "##Running a loop through all the urls to get every piece of data in one list\n",
    "##scraping each url\n",
    "for url in urls:\n",
    "    webpage = requests.get(url)\n",
    "    webpage_content = webpage.content\n",
    "    soup = BeautifulSoup(webpage_content,'html.parser')\n",
    "    table_rows = soup.find_all('td')\n",
    "    #pulling just the player data, which starts on the 20th entry. \n",
    "    #We also want to add some kind of date identifier so for now im doing the url since it has the week in it\n",
    "    for row in table_rows[20:]:\n",
    "        rows.append([row.get_text(),url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I saved what I scraped as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create data frame\n",
    "sample_frame = pd.DataFrame.from_records(rows).reset_index()\n",
    "##save to csv\n",
    "sample_frame.to_csv('All_nfl_rows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above text can be uncommented if you ever need to pull the information again, for example in week 3. The below information is now pulling from the \"All_nfl_rows\" CSV I saved, which has all data from each week on my local and is regularly updated using the above script.<br><br>\n",
    "From here on, I'm going to approach the problem using that static csv, starting with creating a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create frame from saved csv\n",
    "new_frame = pd.read_csv('All_nfl_rows.csv')\n",
    "#rename columns\n",
    "new_frame.columns = ['Row','ID','Data','URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up column with urls so it just has date\n",
    "dates = new_frame['URL'].map(lambda x: x.replace('http://rotoguru1.com/cgi-bin/fyday.pl?week=','')\\\n",
    "                             .replace('&game=fd',''))\n",
    "new_frame['Week'] = dates\n",
    "\n",
    "#create separate frame that removes all the url columns\n",
    "date_frame = new_frame[['Data','Week']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything from here to the next markup is me clearing out rows that were in the table but did not have player information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7098"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to remove some ads and nav stuff here by \n",
    "#converting to a series, finding the ones that match, and adding back to the table\n",
    "find_Ads = date_frame['Data']\n",
    "#create series that has 0 for what matches the ads\n",
    "ads_found = find_Ads.str.find('RotoGuru')\n",
    "#add column to table with 0's\n",
    "date_frame['Ad'] = ads_found\n",
    "#create new table with those rows with zero gone\n",
    "no_ads = date_frame[date_frame['Ad'] != 1].reset_index(drop=True)\n",
    "len(no_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating to remove Jump To:\n",
    "#converting to a series, finding the ones that match, and adding back to the table\n",
    "find_jump = no_ads['Data']\n",
    "#create series that has 0 for what matches the text\n",
    "jump_found = find_jump.str.find('Jump to:')\n",
    "#add column to table with 0's\n",
    "no_ads['Remove'] = jump_found\n",
    "#create new table with those rows with zero gone\n",
    "jump_gone = no_ads[no_ads['Remove'] !=0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#There's a term 'Unlisted' that pops up occasionally and breaks everything in the NBA version of this page.\n",
    "#I'm clearing that here to be safe\n",
    "find_unlisted = jump_gone['Data']\n",
    "#create series that has a 0 for where it says unlisted\n",
    "unlisted_found = find_unlisted.str.find('Unlisted')\n",
    "#add column to table with 0's\n",
    "jump_gone['Z'] = unlisted_found\n",
    "#create new table with those rows removed\n",
    "unlisted_gone = jump_gone[jump_gone['Z'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few headers in our table that weren't in the nba one. I'm clearing those out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating for Opp \n",
    "find_opp = unlisted_gone['Data']\n",
    "#create series that has a 0 for where it says unlisted\n",
    "opp_found = find_opp.str.find('Opp.')\n",
    "#add column to table with 0's\n",
    "unlisted_gone['F'] = opp_found\n",
    "#create new table with those rows removed\n",
    "opp_gone = unlisted_gone[unlisted_gone['F'] != 0].reset_index(drop=True)\n",
    "#Repeating for Wide Recievers\n",
    "find_wide = opp_gone['Data']\n",
    "#create series that has a 0 for where it says unlisted\n",
    "wide_found = find_wide.str.find('Wide Receivers')\n",
    "#add column to table with 0's\n",
    "opp_gone['X'] = wide_found\n",
    "#create new table with those rows removed\n",
    "wide_gone = opp_gone[opp_gone['X'] != 0].reset_index(drop=True)\n",
    "#Repeating for \\n\n",
    "find_n = wide_gone['Data']\n",
    "##create series that has a 0 for where it says unlisted\n",
    "n_found = find_n.str.find('\\n\\n\\n\\n')\n",
    "##add column to table with 0's\n",
    "wide_gone['W'] = n_found\n",
    "##create new table with those rows removed\n",
    "n_gone = wide_gone[wide_gone['W'] != 0].reset_index(drop=True)\n",
    "##Repeating for Score\n",
    "find_score = n_gone['Data']\n",
    "##create series that has a 0 for where it says unlisted\n",
    "score_found = find_n.str.find('Score')\n",
    "##add column to table with 0's\n",
    "n_gone['R'] = score_found\n",
    "##create new table with those rows removed\n",
    "score_gone = n_gone[n_gone['R'] != 0].reset_index(drop=True)\n",
    "##Repeating for Tight Ends\n",
    "find_tight = score_gone['Data']\n",
    "##create series that has a 0 for where it says unlisted\n",
    "tight_found = find_tight.str.find('Tight Ends')\n",
    "##add column to table with 0's\n",
    "score_gone['S'] = tight_found\n",
    "##create new table with those rows removed\n",
    "tight_gone = score_gone[score_gone['S'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few other terms that kept popping up - I ended up finding a more efficient way to clear them out but have left both methods in this notebook so I can see the two different options. str.contains() would be a little more dangerous for a two letter arrangement for common than QB, especially if it wasn't uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##There's a subtable headers that aren't player data. we are getting rid of most those here.\n",
    "##creates list of all the words I want to find and get rid of\n",
    "sub = ['QB','Points','Team','Salary','Unlisted','Running Backs',\\\n",
    "       'Kickers','Defenses']\n",
    "pattern = '|'.join(sub)\n",
    "\n",
    "tight_gone['gone'] = tight_gone['Data'].str.contains(pattern, case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove any rows where we found those subtable headers\n",
    "clean_table = tight_gone[tight_gone['gone'] != True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create series with the data\n",
    "just_data = clean_table[['Data','Week']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging data and date in a column so\n",
    "#I can then hopefully turn each one into a series and then just have the date once at the end.\n",
    "just_data['merge_date'] = just_data['Data'].astype(str)+'|'+just_data['Week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning my merged column into a list so I can run a comprehension and then add the date to the end of a player row\n",
    "just_datas = just_data['merge_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning series into a list so we can do some stuff\n",
    "data_list = list(just_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used this loop to create sublists per player based on each player entry having 5 columns\n",
    "##each row was 5 entries. This gets thrown off very easily though so we need to be careful to remove all other data\n",
    "##which we have already done above\n",
    "players = [data_list[x:x+5] for x in range(0, len(data_list), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a row per player, but every piece of data has the date. \n",
    "#So first I'm pulling the date out and adding it to the end of every sublist\n",
    "for player in players:\n",
    "    player.insert(0,player[0].split('|')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list that has each player entry as its own record without date. \n",
    "#Note - we already pulled the date and added it to the end of the sublist.\n",
    "#If you haven't done that I recommend you do so first\n",
    "player_rows =[]\n",
    "for player in players:\n",
    "    if len(player) == 6:\n",
    "        player_rows.append([player[0],\\\n",
    "                            player[1].split('|')[0],\\\n",
    "                            player[2].split('|')[0],\\\n",
    "                            player[3].split('|')[0],\\\n",
    "                            player[4].split('|')[0],\\\n",
    "                            player[5].split('|')[0],\\\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame = pd.DataFrame.from_records(player_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_frame.columns = ['Week','Name','Team','Opponent','Fanduel_Points','Fanduel_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace currency symbols in column so we can make it an integer\n",
    "sample_frame['Fanduel_Price'] = sample_frame['Fanduel_Price'].str.replace(',', '')\n",
    "sample_frame['Fanduel_Price'] = sample_frame['Fanduel_Price'].str.replace('$', '')\n",
    "#Turn the column to integers\n",
    "sample_frame['Fanduel_Price'] = pd.to_numeric(sample_frame['Fanduel_Price'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've got a frame with consistent columns, I'm going to clean things up by making some columns a float and also splitting out the opponent so we can see home vs away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no price listed for some players. i am removing them since the whole goal is to see who exceeds their price\n",
    "df1 = sample_frame[~sample_frame['Fanduel_Price'].isna()].reset_index(drop=True)\n",
    "#Convert Fanduel_Points to float\n",
    "df1['Fanduel_Points'] = pd.to_numeric(df1['Fanduel_Points'])\n",
    "#create column for home vs away and updated column for opponent\n",
    "df1['Split'] = df1['Opponent'].str[0]\n",
    "df1['Home'] = df1.Split.apply(lambda x: 'Home' if str(x) == 'v' else 'Away')\n",
    "df1['Foe'] = df1['Opponent'].str[1:]\n",
    "#get rid of periods in home v away\n",
    "df1['Opponent'] = df1['Foe'].str.replace('. ','')\n",
    "#get rid of some carrots that are appearing\n",
    "df1['Name'] = df1['Name'].str.replace('^','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing Split column\n",
    "del df1['Split']\n",
    "del df1['Foe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have a frame with the data for all players week to week. I should now be able to refresh this information every week. as long as I scrape the new scores first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Opponent</th>\n",
       "      <th>Fanduel_Points</th>\n",
       "      <th>Fanduel_Price</th>\n",
       "      <th>Home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wilson, Russell</td>\n",
       "      <td>sea</td>\n",
       "      <td>atl</td>\n",
       "      <td>31.78</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rodgers, Aaron</td>\n",
       "      <td>gnb</td>\n",
       "      <td>min</td>\n",
       "      <td>30.76</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Josh</td>\n",
       "      <td>buf</td>\n",
       "      <td>nyj</td>\n",
       "      <td>28.18</td>\n",
       "      <td>7900.0</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Jackson, Lamar</td>\n",
       "      <td>bal</td>\n",
       "      <td>cle</td>\n",
       "      <td>27.50</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Murray, Kyler</td>\n",
       "      <td>ari</td>\n",
       "      <td>sfo</td>\n",
       "      <td>27.30</td>\n",
       "      <td>7700.0</td>\n",
       "      <td>Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>3</td>\n",
       "      <td>Washington</td>\n",
       "      <td>was</td>\n",
       "      <td>cle</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>jac</td>\n",
       "      <td>mia</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>3</td>\n",
       "      <td>New York J</td>\n",
       "      <td>nyj</td>\n",
       "      <td>ind</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>3</td>\n",
       "      <td>New York G</td>\n",
       "      <td>nyg</td>\n",
       "      <td>sfo</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>3</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>nor</td>\n",
       "      <td>gnb</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1302 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Week             Name Team Opponent  Fanduel_Points  Fanduel_Price  Home\n",
       "0       1  Wilson, Russell  sea      atl           31.78         8400.0  Away\n",
       "1       1   Rodgers, Aaron  gnb      min           30.76         7600.0  Away\n",
       "2       1      Allen, Josh  buf      nyj           28.18         7900.0  Home\n",
       "3       1   Jackson, Lamar  bal      cle           27.50         9300.0  Home\n",
       "4       1    Murray, Kyler  ari      sfo           27.30         7700.0  Away\n",
       "...   ...              ...  ...      ...             ...            ...   ...\n",
       "1297    3       Washington  was      cle            1.00         3900.0  Away\n",
       "1298    3     Jacksonville  jac      mia            0.00         3800.0  Home\n",
       "1299    3       New York J  nyj      ind            0.00         3800.0  Away\n",
       "1300    3       New York G  nyg      sfo           -2.00         3600.0  Home\n",
       "1301    3      New Orleans  nor      gnb           -3.00         4300.0  Home\n",
       "\n",
       "[1302 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going ahead and saving this for a safe template\n",
    "df1.to_csv('nfl_weekly_fanduel_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook (\"Manipulating Fanduel football\"), I'm going to try to add some columns to this data, specifically position."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
